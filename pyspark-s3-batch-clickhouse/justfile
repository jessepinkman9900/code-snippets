[private]
default:
    just --list

# install dependencies/tools
init:
  mise install
  pre-commit install
  # dotenvx
  curl -sfS https://dotenvx.sh/install.sh | sh
  # goose cli
  go install github.com/pressly/goose/v3/cmd/goose@v3.24.0

[group('data')]
load profile='accelerate-lambdas' region='ap-northeast-1':
  mkdir -p .data-l1/hl-mainnet-node-data/explorer_blocks/0 && aws s3 sync s3://hl-mainnet-node-data/explorer_blocks/0/ .data-l1/hl-mainnet-node-data/explorer_blocks/0 --request-payer requester --profile {{profile}} --region {{region}}
  du -sh .data-l1

[group('docker')]
up env='test': down
  dotenvx run -f .env.{{env}} -- docker compose up -d

[group('docker')]
down:
  docker compose down -v
  rm -rf .data-clickhouse

[group('jupyter-notebook')]
run:
  cd notebook && uv run jupyter lab

# create package for spark job - python venv zip, jars & pyspark script
[group('aws-emr')]
pkg:
  rm -rf target && mkdir -p target && cp *.py target && DOCKER_BUILDKIT=1 docker build -f Dockerfile --output ./target .
  mkdir -p target/jars && chmod +x download-jars.sh && ./download-jars.sh && mv *.jar target/jars

# copies target folder to s3 & submits job using aws cli
[group('aws-emr')]
submit:
  chmod +x submit-job.sh && dotenvx run -f .env -- ./submit-job.sh

# pkg & submit
[group('aws-emr')]
clean-submit: pkg submit

# terraform init
[group('terraform')]
init env='test':
  cd infra/terraform/workspaces/{{env}} && dotenvx run -f .env -- terraform init

# terraform plan
[group('terraform')]
plan env='test':
  cd infra/terraform/workspaces/{{env}} && dotenvx run -f .env -- terraform plan

# terraform apply
[group('terraform')]
apply env='test':
  cd infra/terraform/workspaces/{{env}} && dotenvx run -f .env -- terraform apply

# terraform destroy
[group('terraform')]
destroy env='test':
  cd infra/terraform/workspaces/{{env}} && dotenvx run -f .env -- terraform destroy
